Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.




Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  2.00s/it]
Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.
No chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Traceback (most recent call last):
  File "/home01/x2889a02/GoodLiar/main_on_the_fly.py", line 141, in <module>
    main()
  File "/home01/x2889a02/GoodLiar/main_on_the_fly.py", line 121, in main
    rewards=reward_liar(data, evaluator_path),
  File "/home01/x2889a02/GoodLiar/main_on_the_fly.py", line 39, in reward_liar
    qa = pipeline("question-answering", model=model_name)
  File "/scratch/x2889a02/.conda/envs/demo/lib/python3.8/site-packages/transformers/pipelines/__init__.py", line 977, in pipeline
    raise Exception(
Exception: Impossible to guess which tokenizer to use. Please provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer.
Traceback (most recent call last):
  File "/home01/x2889a02/GoodLiar/main_on_the_fly.py", line 141, in <module>
    main()
  File "/home01/x2889a02/GoodLiar/main_on_the_fly.py", line 121, in main
    rewards=reward_liar(data, evaluator_path),
  File "/home01/x2889a02/GoodLiar/main_on_the_fly.py", line 39, in reward_liar
    qa = pipeline("question-answering", model=model_name)
  File "/scratch/x2889a02/.conda/envs/demo/lib/python3.8/site-packages/transformers/pipelines/__init__.py", line 977, in pipeline
    raise Exception(
Exception: Impossible to guess which tokenizer to use. Please provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer.
{
    "method": {
        "name": "ilqlconfig",
        "tau": 0.7,
        "gamma": 0.99,
        "cql_scale": 0.1,
        "awac_scale": 1,
        "alpha": 0.001,
        "beta": 0,
        "steps_for_target_q_sync": 5,
        "two_qs": true,
        "gen_kwargs": {
            "max_new_tokens": 56,
            "top_k": 20,
            "beta": 1,
            "temperature": 1.0
        }
    },
    "model": {
        "model_path": "gpt2",
        "model_arch_type": "causal",
        "num_layers_unfrozen": -1,
        "peft_config": null,
        "model_extra_configs": {}
    },
    "optimizer": {
        "name": "adamw",
        "kwargs": {
            "lr": 5e-05,
            "betas": [
                0.9,
                0.95
            ],
            "eps": 1e-08,
            "weight_decay": 1e-06
        }
    },
    "scheduler": {
        "name": "cosine_annealing",
        "kwargs": {
            "T_max": 1000000000000.0,
            "eta_min": 5e-05
        }
    },
    "tokenizer": {
        "tokenizer_path": "gpt2",
        "padding_side": "left",
        "truncation_side": "right",
        "tokenizer_extra_configs": {}
    },
    "train": {
        "total_steps": 1000,
        "seq_length": 64,
        "epochs": 10,
        "batch_size": 10,
        "checkpoint_interval": 1000,
        "eval_interval": 100,
        "pipeline": "PromptPipeline",
        "trainer": "AccelerateILQLTrainer",
        "trainer_kwargs": {},
        "project_name": "trlx",
        "run_name": null,
        "entity_name": null,
        "group_name": null,
        "checkpoint_dir": "ckpts_liar",
        "rollout_logging_dir": "ckpts_liar",
        "save_best": true,
        "save_optimizer": true,
        "resume_from_checkpoint": null,
        "tracker": null,
        "logging_dir": null,
        "tags": [],
        "seed": 1000,
        "minibatch_size": null
    }
}